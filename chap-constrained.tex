%\cnote{Discuss what those can be (local privacy, noisy channels, linear of partial measurements, etc). Focus on communication constraints ($\numbits$-bit measurements). Talk about adaptive schemes?}

To conclude this survey, we will venture outside the usual sampling setting, and consider the following question: \emph{what happens when the algorithm does not get to see the $\ns$ \iid samples?}

This may seem absurd at first: well, then, the algorithm is in trouble, isn't it? Yet, this type of question does in fact capture many natural (or interesting) settings. Among others:
\begin{description}
	\item[Communication constraints:] The data is divided among $\ns$ users, each holding a single sample\footnote{One could also, of course, consider scenarios where users hold multiple samples each; it is, however, a little more complicated to handle.} (observation), and a central server seeks to perform the testing task. Unfortunately, the users each have a stringent \emph{bandwidth constraint}, preventing them from sending their full data point to the server: instead, they are limited to only send $\numbits$ of information.
	\item[Limited measurements:] Data is hard to measure, and physical devices (or social incentive mechanisms) are imperfect or restricted. For instance, it may only be possible to perform a specific type of one-bit measurement to each data point: fix a threshold, and only learn whether the value is greater. Or it may be the case that sensors can be very accurate either for higher temperatures, or lower ones, but not both: which ones to choose to deploy?
	\item[Quantization:] Very often, the underlying signal is continuous, but the measurement is intrinsically discrete. Which quantization scheme to choose? Should it be chosen once and for all, or should various quantization schemes be combined, different for distinct measurements?
	\item[Privacy:] Sometimes the data is not only distributed across many users, but also sensitive: \eg medical data, location information, or financial records. The users, while willing to send some information to the central server in order to perform the testing task, seek to preserve the privacy of their personal data. This is captured, \eg by the framework of (local) differential privacy~\citep{DworkMNS06,KLNRS:11,DJW:13:FOCS}, which guarantees (in a formal sense) that nobody~--~even the central server~--~can infer too much about any single user's data point.
	\item[Streaming and memory-limited devices:] In some cases, the measurements are performed (or the data observed) sequentially by a device with limited working memory. The algorithm thus cannot store the totality of the dataset before performing computations on it, but instead must maintain a small ``sketch'' of the samples seen so far, and base its final output on this sketch only.
	\item[Noisy channels:] We conclude this (non-exhaustive) list with the example of settings where the measurements are performed locally, and sent to a central entity for processing through a noisy communication channel. Each such transmission can be subject to data corruption, either through random noise or adversarially.
\end{description}
This is a \emph{short} concluding chapter, and we will not cover all (or, indeed, most) of the above applications; the interested reader is referred to~\cref{sec:constrained:notes} for a few pointers. Instead, we will focus on the first setting, that of communication constraints, where each of $\ns$ users observes a single (independent) sample from the same unknown distribution $\p\in\distribs{\ab}$, but must abide by a tight communication budget of $\numbits \ll \log\ab$ bits. We will also focus on upper bounds (algorithms), and on our by-now-familiar example of uniformity testing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setting(s), and the devil lurking in the details}
Before doing anything else, it is important to define~--~and discuss~--~this communication-limited setting. As mentioned, we have $\ns$ \iid samples $X_1,\dots,X_\ns$ drawn from the same unknown distribution $\p\in\distribs{\ab}$. These samples are distributed among $\ns$ users, where user $i$ observes sample $X_i$ and, from it, computes an $\numbits$-bit message $Y_i\in\{0,1\}^\numbits$ and sends it to the central server.

Upon observing these $\ns$ messages $Y_1,\dots, Y_\ns$, the central server (which does not have itself any sample from $\p$) runs an algorithm to test whether $\p$ is uniform, or $\dst$-far from it, and must be correct with probability $1-\errprob$. 

Note that we will implicitly assume afterwards that $\numbits\leq \log\ab$,\footnote{To avoid this implicit assumption, we could just replace $\numbits$ by $\numbits\land\log\ab$ in every statement, but that is neither nice to read nor to write.} since otherwise each user can simply send their full sample (which only takes $\log\ab$ bits to encode) to the server, and we are back in our familiar setting, with a tight $\Theta(\sqrt{\ab}/\dst^2)$ sample complexity bound for uniformity testing. Given this new communication constraint, we know that the new bound on $\ns$, which is both the number of users and the number of samples, will be \emph{at least} $\Theta(\sqrt{\ab}/\dst^2)$; but, most likely, higher (and should depend on $\numbits$).

\begin{figure}\centering
\input{fig-ic}
\caption{Depiction of the communication-constrained setting, in its (almost) full, glorious generality. The orange dashed box highlights that, in the public-coin setting, the users can jointly randomize their messages even though they do not directly communicate.   The red dotted arrows indicate that, in the (sequentially) interactive setting, user $i$ observes the messages $Y_1,\dots,Y_{i-1}$, and can choose their own message $Y_i$ based on those (as well as $X_i$).\label{fig:models:distributed}}
\end{figure}

\paragraph{But what do the users know, exactly?} We assume that the users are not given the parameters $\dst,\errprob$, but know $\ab$: since they perform the measurement, they probably are aware of the domain of the data. They also know the details the protocol they must follow to compute the message to send (so, in particular, we can assume they know the total number of users $\ns$, if needed), and are given a way to identify themselves in this protocol (\ie each user has a unique ID).

\paragraph{But what do the users share, exactly?} Now, we said that user $i$ (for $1\leq i\leq \ns$) computes their message $Y_i\in\{0,1\}^\numbits$ from $X_i$. Let us make it a bit more formal: user $i$ is equipped with a (possibly randomized) function $W_i\colon \domain\to\{0,1\}^\numbits$, which is decided ahead of time as part of the overall protocol the users and server follow; and sets $Y_i \eqdef W_i(X_i)$.

Note that we allow $W_i$ to differ across users: we do not require that they all use the same mapping from observation to messages. We also allow it to be randomized, which, as we will see, can be quite useful: but this raises the question of \emph{which random seed is used}. Are $W_1,\dots,W_\ns$ randomized independently (\ie each $W_i$ has its own, ``private'' random seed $R_i$, independent of both the inputs $X_1,\dots,X_\ns$ and of the other $R_j$'s)? Are they randomized jointly (\ie each $W_i$ has its own, ``private'' random seed $R_i$ as before, \emph{and} a shared random seed $U$ which all users and the server observe~--~still independent of the inputs $X_1,\dots,X_\ns$, of course)? Or do we go even further, and do we allow $W_i$ to depend on the messages previously sent, that is, we allow user $i$ to observe $Y_1,\dots,Y_{i-1}$ before sending their own message?

The answer is: any of the above, choose your own adventure. Each of the 3 settings above captures a different scenario, and has its own pros and cons:
\begin{itemize}
	\item Independent randomization (no common random seed, only private randomness), and no seeing previous messages: this is the \emph{private-coin} setting, sometimes called private-coin ``simultaneous message-passing'' (SMP). It is possibly the simplest to implement, which is a clear advantage; however, it is also the most restrictive, and thus we can expect the sample complexity to be the worst in this setting.
	\item Joint randomization (common random seed available, on top of private randomness), and no seeing previous messages: this is the \emph{public-coin} setting. It makes sense in scenarios where the server can broadcast a message to all users, or when some earlier synchronization between devices has been performed ahead of time. More permissive than the private-coin setting, so we can hope to achieve better sample complexity.
	\item \emph{Sequentially interactive:} common random seed available, on top of private randomness, \emph{and} users get to see the messages sent by users before them. This is the most challenging to implement, and can come at the cost of delays and latencies; still, it might also allow for better sample complexity, so\dots{} maybe things balance out?
\end{itemize}
There are even more permissive settings (\eg the so-called ``blackboard model,'' also known as \emph{tree protocols}), but this is already quite a lot to absorb. The three settings discussed above are depicted (with more or less success) in~\cref{fig:models:distributed}.

\paragraph{But what can the users \emph{achieve}, exactly?} In the next section,~\cref{sec:simulate:infer}, we will see a simple, yet powerful technique, \emph{simulate-and-infer}, which leads to a $O(\ab^{3/2}/2^\numbits\dst^2)$ sample complexity for uniformity testing in the private-coin setting (\cref{theo:private-coin}). Viewed differently, this is 
\begin{equation}
	\label{eq:private:coin}
		\underbrace{\frac{\ab}{2^\numbits}}_{\substack{\text{Cost of distributed}\\\text{setting}}}\cdot \underbrace{\frac{\sqrt{\ab}}{\dst^2}}_{\substack{\text{Cost in the}\\\text{centralized setting}}}
\end{equation}
and happens to be optimal (no private-coin protocol for uniformity testing can do better, as a function of $\ab,\dst,\numbits$). As a sanity check, the first factor disappears when $\numbits = \log\ab$, which is comforting.\smallskip

We will then see that, somewhat suprisingly, \emph{public randomness helps a lot for testing}. In~\cref{sec:domain:compression}, using the \emph{domain compression} technique introduced in~\cref{ssec:random:hashing}, we will see that public-coin protocols can achieve the much better $O(\ab/2^{\numbits/2}\dst^2)$ sample complexity for uniformity testing; or, equivalently,
\begin{equation}
		\underbrace{\sqrt{\frac{\ab}{2^\numbits}}}_{\substack{\text{Cost of distributed}\\\text{setting}}}\cdot \underbrace{\frac{\sqrt{\ab}}{\dst^2}}_{\substack{\text{Cost in the}\\\text{centralized setting}}}
\end{equation}
%(Again, the first term disappears when $\numbits$ reaches $\log\ab$.) 
What is perhaps even more surprising is that this is also optimal, \emph{even} when one allows sequentially interactive protocols! So, public randomness helps; but interaction? Not so much.


Oh, one last detail: to make things slightly more confusing: since we are in a distributed setting, we now talk about testing \emph{protocols}, not algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulate-and-Infer}
  \label{sec:simulate:infer}
 We will focus here on proving the upper bound of the following theorem (the lower bound, unfortunately, would probably require another chapter, and more coffee than the author currently has at his disposal):
\begin{theorem}
  \label{theo:private-coin}
There exists a private-coin testing protocol for uniformity under $\numbits$-bits communication constraints using $\ns(\ab,\dst,\numbits, 1/3) = O(\ab^{3/2}/(2^\numbits \dst^2))$ users. Moreover, this is optimal among all such private-coin protocols.
\end{theorem}
To establish this, we will describe a very simple technique, \emph{simulate-and-infer}, which essentially allows users to simulate, \emph{via} a distributed private-coin protocols, honest-to-goodness new \iid samples from the actual (unknown) distribution $\p$, even though none of them has the communication budget to send their actual samples. Now, if it takes $\ab/2^\numbits$ users to generate a single new sample at the server, then we can just repeat this in parallel on disjoint groups of users, until the server has enough samples to run they favorite non-distributed uniformity testing algorithm from~\cref{sec:uniformity}. Hence the name of the technique: simulate samples, \emph{then} infer from them\dots which leads to the sample complexity bound of~\cref{eq:private:coin} (and~\cref{theo:private-coin}).\smallskip

\noindent Let us formally state what this ``simulation'' technique is about.
\begin{theorem}[Distributed Simulation]
	\label{theo:simulate:infer}
For any $1\leq \numbits \leq \log\ab$, there exists a private-coin protocol which lets the server simulate an expected $\ns' \asymp \ns 2^\numbits/\ab$ \iid samples from an unknown distribution $\p\in\distribs{\ab}$, given the $\numbits$-bit messages from $\ns$ users, each holding an independent sample from this $\p$.
\end{theorem}

We will not prove this theorem in detail, but instead give the main idea, starting with the case $\numbits=1$, showing how to generate \emph{one} sample from $\ns=2\ab$ users. The generalisation to $\numbits\geq 2$ is then a little fastidious, but relatively straightforward: see~\cref{exo:simulate:infer}. 

Start by partitioning these $2\ab$ users in pairs: say, users $2i-1$ and $2i$, for $1\leq i\leq \ab$. Pair $i$ will be ``assigned'' element $i$ of the domain, and the one-bit message they will send are just the indicators
\[
	Y_{2i-1} \eqdef \indic{X_{2i-1}=i}, \qquad Y_{2i} \eqdef \indic{X_{2i}=i}
\]
of whether their respective sample fell on their assigned element $i$. The server, upon receiving these $\ns=2\ab$ messages, will check the following two conditions:
\begin{itemize}
	\item there exists one, and only one, pair $(2i-1,2i)$ of users for which the ``even'' user sent $1$ ($Y_{2i}=1$); and
	\item for this pair $(2i-1,2i)$, the ``odd'' user sent $0$ ($Y_{2i-1}=1$).
\end{itemize}
If those two conditions do not simultaneously hold (either at least two even users sent $1$, or the odd user from the pair sent $1$ as well), then the server aborts (does not output any sample, but outputs, say, $\bot$ instead). Otherwise, the server outputs $i$ as its sample. This procedure may seem arbitrary, but it is then not too hard to check that the probability that $i\in[\ab]$ is output is then given by
\begin{equation}
	\label{eq:simulate:infer:babyversion}
	\bPr{ \text{output is } i } = \p(i)\!\!\!\!\prod_{\substack{1\leq j\leq \ab\\j\neq i}} \!\!(1-\p(j)) \cdot (1-\p(i))
	= \p(i)\prod_{j=1}^\ab (1-\p(j))
\end{equation}
where (a)~the first term, $\p(i)$, is the probability that user $2i$ sends $1$, (b)~the second term, $\prod_{j\neq i} (1-\p(j))$, is the probability that no other user $2j$ sends $1$, and (c)~the last term, $1-\p(i)$, is the probability that user $2i-1$ sends $0$.

Squinting a bit at~\cref{eq:simulate:infer:babyversion}, we see that $\bPr{ \text{output is } i }\propto \p(i)$ for every $i\in[\ab]$, since $\prod_{j=1}^\ab (1-\p(j))$ does not depend on $i$. This is encouraging! This means that, conditioned on outputting \emph{something}, the server outputs a sample from the right distribution.

To conclude, it only remains to show that the probability to output \emph{something} (which, by summing~\cref{eq:simulate:infer:babyversion} over $i\in[\ab]$, is exactly this quantity $\prod_{j=1}^\ab (1-\p(j))$) is not too bad, say, at least a constant. So we need a good lower bound: using the rabbit-out-of-a-hat inequality $1-u \geq 1/4^u$ (which holds for $0\leq u \leq 1/2$), we can write
\begin{equation}
 \prod_{j=1}^\ab (1-\p(j))
\geq \prod_{j=1}^\ab \frac{1}{4^{\p(j)}} = \frac{1}{4}
\end{equation}
as long as $\norminf{\p}\leq 1/2$. \emph{Which we do not know.} But we can enforce it, using private randomness from each user, and a factor 2 in the number of users: namely, start by mapping $\p\in\distribs{\ab}$ to $\Phi(\p)\in\distribs{2\ab}$, via the simple randomized mapping $\Psi$ which, on input $i\in[\ab]$, returns either $i$ or $i+\ab$, each with probability $1/2$. This only needs private randomness from each user, preserves the total variation distances, and does not require any knowledge of $\p$; but now, $\norminf{\Phi(\p)} = \norminf{\p}/2 \leq 1/2$, so the above argument goes through~--~only replacing $\ab$ by $2\ab$ (and so, $\ns=2\ab$ users by $4\ab$ users).

What we showed can be summarized as follows:
\begin{lemma}[Distributed Simulation, Baby Version]
	\label{theo:simulate:infer:babyversion}
There exists a private-coin protocol which lets the server simulate an expected $\ns' \geq \frac{1}{4}\flr{\frac{\ns}{4\ab}}$ \iid samples from an unknown distribution $\p\in\distribs{\ab}$, given the $1$-bit messages from $\ns$ users, each holding an independent sample from this $\p$.
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Random hashing and domain compression}
  \label{sec:domain:compression}
We have now seen a general technique to obtain private-coin protocols under communication constraints. What about \emph{public-coin} protocols? How do we take advantage of this common random seed the users have access to? Ironically, the answer to this can be found nearly a hundred pages ago, in~\cref{ssec:random:hashing}; and, more specifically,~\cref{theo:random:dct:hashing}, which will allow us to establish (the upper bound of) the following result:
\begin{theorem}
  \label{theo:public-coin}
There exists a public-coin testing protocol for uniformity under $\numbits$-bits communication constraints using $\ns(\ab,\dst,\numbits, 1/3) = O(\ab/(2^{\numbits/2} \dst^2))$ users. Moreover, this is optimal, even among \emph{interactive} protocols.
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Historical notes}
  \label{sec:constrained:notes}
Limited randomness (note that Algo 5 mentioned 4-wise randomness)

simulate-and-infer for estimation, and nonparametric estimation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercises}
identity reduction
error amplification
generalize to different $\ell_i$

an "expected" -> fill in the details.