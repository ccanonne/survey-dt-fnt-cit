We here focus on distributions over discrete domains; all of the stated results do extend to the continuous settings, replacing ratios by Radon--Nikodym derivatives and sums by suitable integrals.

We briefly recall the definitions of the distance measures between probability distributions we will use here. This list is by no means exhaustive, of course: there be (many more) dragons. 
\begin{definition}
For two probability distributions $\p_1,\p_2$ over the same domain $\domain$, the \emph{Kullback--Leibler divergence} (in nats), \emph{chi--square divergence}, and \emph{Hellinger distance} are given by
\begin{align}
	\kldiv{\p_1}{\p_2} &= \sum_{x\in\domain} \p_1(x) \ln \frac{\p_1(x)}{\q_1(x)} \\
	\chisquare{\p_1}{\p_2} &= \sum_{x\in\domain} \frac{(\p_1(x)-\q_1(x))^2}{\q_1(x)} \\
	\hellinger{\p_1}{\p_2} %&= \Paren{\frac{1}{2}\sum_{x\in\domain} \Paren{\sqrt{\p_1(x)}-\sqrt{\q_1(x)}}^2}^{1/2}  \notag\\
	&= \frac{1}{\sqrt{2}}\normtwo{\sqrt{\p}-\sqrt{\q}}\,,
\end{align}
with the convention that $0\ln 0 = 0$. Note that the first two are not symmetric, do not satisfy the triangle inequality, and are unbounded.
\end{definition}

Importantly, TV distance, squared Hellinger, KL divergence, and chi-square divergence are all instances of \emph{$f$-divergences}, which directly endows them with many desirable properties~--~among which joint convexity and the data-processing inequality (\cref{fact:dpi}).

Squared Hellinger, KL divergence, and chi-square divergence also ``tensorize'' nicely: specifically, for any product probability distributions $\p_1\otimes\cdots\otimes\p_n$ and $\q_1\otimes\cdots\otimes\q_n$, we have
\begin{align}
\kldiv{\p_1\otimes\cdots\otimes\p_n}{\q_1\otimes\cdots\otimes\q_n} 
&= \sum_{i=1}^n \kldiv{\p_i}{\q_i} \\
\chisquare{\p_1\otimes\cdots\otimes\p_n}{\q_1\otimes\cdots\otimes\q_n} 
&= \prod_{i=1}^n \Paren{1+\chisquare{\p_i}{\q_i}} - 1
\end{align}
and
\begin{align}
\hellinger{\p_1\otimes\cdots\otimes\p_n}{\q_1\otimes\cdots\otimes\q_n}^2 
&= 1 - \prod_{i=1}^n (1-\hellinger{\p_i}{\q_i}^2) \notag\\
&\leq \sum_{i=1}^n \hellinger{\p_i}{\q_i}^2
\end{align}
while TV distance is much less cooperative, only giving the weaker
\begin{equation}
\totalvardist{\p_1\otimes\cdots\otimes\p_n}{\q_1\otimes\cdots\otimes\q_n} \leq \sum_{i=1}^n \totalvardist{\p_i}{\q_i}
\end{equation}
(typically much looser, loosing up to a factor $\sqrt{n}$ compared to what one would get via, say, Hellinger).

We now state (and prove) several useful lemmas relating these various distance measures.
\begin{lemma}
  \label{app:distances:hellinger}
For every $\p,\q$ on $\domain$, 
  \[
    \hellinger{\p}{\q}^2 \leq \totalvardist{\p}{\q} \leq \sqrt{2}\hellinger{\p}{\q}\,.
  \]
\end{lemma}
\begin{proof}
  Let us first prove the left side. Using $a-b=(\sqrt{a}-\sqrt{b})(\sqrt{a}+\sqrt{b})$,
  \begin{align*}
      \hellinger{\p}{\q}^2
      &= \frac{1}{2}\sum_{x\in\domain} \Paren{\sqrt{\p(x)}-\sqrt{\q(x)}}^2 \\
      &\leq \frac{1}{2}\sum_{x\in\domain} \abs{\sqrt{\p(x)}-\sqrt{\q(x)}}\Paren{\sqrt{\p(x)}+\sqrt{\q(x)}} \\
      &= \frac{1}{2}\sum_{x\in\domain} \abs{\p(x)-\q(x)} = \totalvardist{\p}{\q}\,.
  \end{align*}
 For the right side, we have, by Cauchy--Schwarz and then using the identity $2(a+b) = (\sqrt{a}+\sqrt{b})^2+(\sqrt{a}-\sqrt{b})^2$,
   \begin{align*}
      \totalvardist{\p}{\q} &= \frac{1}{2}\sum_{x\in\domain} \abs{\sqrt{\p(x)}-\sqrt{\q(x)}}\Paren{\sqrt{\p(x)}+\sqrt{\q(x)}}  \\
      &\leq \frac{1}{2}\sqrt{\sum_{x\in\domain} \Paren{\sqrt{\p(x)}-\sqrt{\q(x)}}^2}\sqrt{\sum_{x\in\domain} \Paren{\sqrt{\p(x)}+\sqrt{\q(x)}}^2} \\
      &= \frac{1}{\sqrt{2}}\hellinger{\p}{\q}\sqrt{\sum_{x\in\domain} \Paren{2(\p(x)+\q(x))-\Paren{\sqrt{\p(x)}-\sqrt{\q(x)}}^2 }}\\
      &= \hellinger{\p}{\q}\sqrt{2-\hellinger{\p}{\q}^2 }\,,
  \end{align*}
  which implies the (slightly weaker) inequality we wanted to show.
\end{proof}

\begin{lemma}
  \label{app:distances:chi2:tv}
For every $\p,\q$ on $\domain$, 
  \[
    \totalvardist{\p}{\q}^2 \leq \frac{1}{4}\chisquare{\p}{\q}\,.
  \]
\end{lemma}
\begin{proof}
  By Cauchy--Schwarz, 
  \begin{align*}
      \totalvardist{\p}{\q} &= \frac{1}{2}\sum_{x\in\domain} \abs{\p(x)-\q(x)} \\
      &\leq \frac{1}{2}\sqrt{\sum_{x\in\domain} \frac{(\p(x)-\q(x))^2}{\q(x)}}\sqrt{\sum_{x\in\domain} \q(x)} \\
      &= \frac{1}{2}\sqrt{\chisquare{\p}{\q}}\,.
  \end{align*}
\end{proof}

\begin{lemma}[Pinsker's Inequality]
  \label{app:distances:pinsker}
For every $\p,\q$ on $\domain$, 
\[
  \totalvardist{\p}{\q} \leq \sqrt{\frac{1}{2}\kldiv{\p}{\q}}\,.
\]
\end{lemma}
This inequality is ``good enough'' for most situations; nonetheless, we state here a lesser known, but stronger result, for when it is not:
\begin{lemma}[Bretagnolles--Huber Bound]
  \label{app:distances:bh}
For every $\p,\q$ on $\domain$, 
\begin{equation}
  \label{eq:bh}
  \totalvardist{\p}{\q} \leq \sqrt{1-e^{-\kldiv{\p}{\q}}}\,.
\end{equation}
In particular, as $\sqrt{1-e^{-x}} \leq 1-\frac{1}{2}e^{-x}$ for $x\geq 0$, this implies
\begin{equation}
  \label{eq:tsybakov}
  \totalvardist{\p}{\q} \leq 1-\frac{1}{2}e^{-\kldiv{\p}{\q}}\,.
\end{equation}
\end{lemma}
We refer the reader to~\citet{Canonne:note:22} or~\citet[Section~2.4.1]{Tsybakov09} for a proof and discussion of this inequality, due to~\citet{BretagnolleH78}.

\begin{lemma}
  \label{lemma:kl:chi2}
For every $\p,\q$ on $\domain$, 
\[
  \kldiv{\p}{\q} \leq \ln(1+\chisquare{\p}{\q}) \leq \chisquare{\p}{\q}
\]
\end{lemma}
\begin{proof}
  The second inequality follows from the standard convexity inequality $\ln(1+x) \leq x$ (for $x>-1$), so it suffices to prove the first. To do so, observe that
  \begin{align*}
      \kldiv{\p}{\q} 
      &= \sum_{x\in\domain} \p(x)\ln\frac{\p(x)}{\q(x)} \\
      &\leq \ln\sum_{x\in\domain}\frac{\p(x)^2}{\q(x)} \tag{Jensen's inequality}\\
      &= \ln(1+\chisquare{\p}{\q})\,,
  \end{align*}
  where we used concavity of the logarithm.
\end{proof}
\noindent Note that~\cref{app:distances:pinsker,lemma:kl:chi2} together imply a weaker version of~\cref{app:distances:chi2:tv}, losing a factor 2.