%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Uniformity testing}
  \label{sec:uniformity}
\cnote{Thorough presentation of the many algorithms for uniformity testing,
with proofs, and relative advantages of each.}

It is known~\citep{Paninski08} that the sample complexity of uniformity testing with distance parameter $\dst\in(0,1]$ is $\Theta(\sqrt{\ab}/\dst^2)$, which is, for a large range of parameters, significantly smaller than the domain size $\ab$. This means that we can reliably infer interesting properties of the distribution after observing only a tiny fraction of the domain elements! That's nice. Now, \emph{how do we perform uniformity testing?}  And what should we keep in mind while doing so?

There are several things to consider in a testing algorithm. To name a few:
\begin{description}
  \item[Data efficiency:] does the algorithm achieve the optimal sample complexity $\Theta(\sqrt{\ab}/\dst^2)$?
  \item[Time efficiency:] how fast is the algorithm to run (as a function of $\ab,\dst$, and the number of samples $\ns$)?
  \item[Memory efficiency:] how much memory does the algorithm require (as a function of $\ab,\dst$, and $\ns$)?
  \item[Simplicity:] is the algorithm simple to describe and implement?
  \item[``Simplicity'':] is the algorithm simple to \emph{analyze}?
  \item[Robustness:] how \emph{tolerant} is the algorithm to breaches of the promise? That is, does it accept distributions which are not \emph{exactly} uniform as well, or is it very brittle?
  \item[Elegance:] This is somewhat subjective, and each of us might have a different view of what it means. If you have strong feelings about this, however, know that you are not alone.
  \item[Generalizable:] Does the algorithm have other features that might be desirable in other settings?
\end{description}

\cref{tab:summary:uniformity:testing} summarizes a few of those criteria.
\begin{table}[ht]\centering\footnotesize
  \def\arraystretch{1.25}%  1 is the default, change whatever you need
% %   \begin{adjustwidth}{0in}{0in}% adjust the L and R margins by 1 inch
  \begin{tabularx}{\textwidth}{|Y|Y|Y|Y|}
  \hline
     & \bf Sample complexity & \bf Notes & \bf References \\\hline
    \bf Collision-based & $\dfrac{\ab^{1/2}}{\dst^2}$ & Tricky & \cite{GoldreichR00,DiakonikolasGPP19} \\\hline
    \bf Unique elements & $\dfrac{\ab^{1/2}}{\dst^2}$ & $\dst \gg 1/\ab^{1/4}$ & \cite{Paninski08} \\\hline
    \bf Modified $\chi^2$ & $\dfrac{\ab^{1/2}}{\dst^2}$ & (None) & \cite{ValiantV17,AcharyaDK15,DiakonikolasKN15} \\\hline
    \bf Empirical distance to uniform & $\dfrac{\ab^{1/2}}{\dst^2}$ & Biased & \cite{DiakonikolasGPP18} \\\hline
    \bf Random binary hashing & $\dfrac{\ab}{\dst^2}$ & Fun (+ fast, small space) & \cite{AcharyaCT19b} \\\hline
    \bf Bipartite collisions & $\dfrac{\ab^{1/2}}{\dst^2}$ & $\dst \gg 1/\ab^{1/10}$ & \cite{DiakonikolasGKR19} \\\hline
    \bf Empirical subset weighting & $\dfrac{\ab^{1/2}}{\dst^2}$ & $\dst \gg 1/\ab^{1/4}$ &  \\\hline
  \end{tabularx}
% %   \end{adjustwidth}
  \caption{\label{tab:summary:uniformity:testing}The current landscape of uniformity testing, based on the algorithms I know of. For ease of reading, we omit the $O(\cdot)$, $\Theta(\cdot)$, and $\Omega(\cdot)$'s from the table: all results should be read as asymptotic with regard to the parameters, up to absolute constants.}
\end{table}

A key insight, that underlies a lot of the algorithms above, is that here \emph{$\lp[2]$ distance is a good proxy for total variation distance}:
\begin{equation}
  \label{eq:relation:l1:l2:cs}
  \totalvardist{\p}{\uniform_\ab} = \frac{1}{2}\normone{\p-\uniform_\ab} \leq \frac{\sqrt{\ab}}{2}\normtwo{\p-\uniform_\ab}
\end{equation}
the inequality being Cauchy--Schwarz. So if $\totalvardist{\p}{\uniform_\ab}>\dst$, then $\normtwo{\p-\uniform_\ab}^2 > 4\dst^2/\ab$ (and, well, if $\totalvardist{\p}{\uniform_\ab}=0$ then $\normtwo{\p-\uniform_\ab}^2=0$ too, of course). Moreover, we have the very convenient fact, specific to the distance to uniform: for any distribution $\p$ over $[\ab]$,
\begin{equation}
  \label{eq:relation:collisionprob:distance:uniform}
  \normtwo{\p-\uniform_\ab}^2 = \sum_{i=1}^\ab (\p(i)-1/\ab)^2  = \sum_{i=1}^\ab \p(i)^2-1/\ab = \normtwo{\p}^2-1/\ab\,,
\end{equation}
so combining the two we get that $\totalvardist{\p}{\uniform_\ab}>\dst$ implies $\normtwo{\p}^2 > (1+4\dst^2)/\ab$.

\begin{remark}
  \label{rk:collision:probability}
  The quantity $\normtwo{\p}^2$ is commonly known as the \emph{collision probability}\index{collision probability} of $\p$, due to the following easy fact: if $X,Y$ are \iid random variables distributed according to $\p$, then
  \begin{equation}
      \probaOf{X=Y} = \sum_{i\in\domain} \probaOf{X=i, Y=i} = \sum_{i\in\domain} \p(i)^2 = \normtwo{\p}^2
  \end{equation}
  (this generalizes to higher-order collisions, with $\norm{\p}_j^j$). It is easy to see, from~\cref{eq:relation:collisionprob:distance:uniform}, that among all probability distributions over a given support size $\ab$ the collision probability is minimized for the uniform distribution: indeed, $\normtwo{\p}^2 = \frac{1}{\ab}+ \normtwo{\p-\uniform_\ab}^2 \geq \frac{1}{\ab}$. \todonote{Discuss R\'enyi entropy? Exercise, historical notes?}
%   Moreover, the collision probability is related to the \emph{R\'enyi entropy of order $2$}, $H_2(\p)$, via the identity
%   $
%     H_2(\p) = -2\log\normtwo{\p}
%   $.
\end{remark}

\begin{remark}
  \label{rk:relation:l1:l2}
  \cref{eq:relation:l1:l2:cs} provides an upper bound on the total variation distance in terms of the $\lp[2]$ distance. Recalling further that $\lp[p]$ norms are monotone (non-increasing) in $p$,\exercise{Prove it. (Exercise sthg)} we further get that $\normone{x}\geq \normtwo{x}$ for any $x\in\R^d$, and therefore
\begin{equation}
  \label{eq:relation:l1:l2:overall}
  \frac{1}{2}\normtwo{\p-\q} \leq \totalvardist{\p}{\q} \leq \frac{\sqrt{\ab}}{2}\normtwo{\p-\q}
\end{equation}
for any two $\p,\q\in\distribs{\ab}$. Although loose by a factor $\sqrt{\ab}$ (where $\ab$ is the domain size), this relation turns out to be surprisingly useful in many occasions.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Collision-based} In view of~\cref{eq:relation:collisionprob:distance:uniform}, a very natural idea is to estimate $\normtwo{\p}^2$, in order to distinguish between (i)~$\normtwo{\p}^2 = 1/\ab$ (uniform) and (ii)~$\normtwo{\p}^2 > (1+4\dst^2)/\ab$ ($\dst$-far from uniform). How to do that? Upon recalling~\cref{rk:collision:probability}, the probability that two independent samples from $\p$ take the same value (a ``collision'') is exactly $\normtwo{\p}^2$. Thus, 
an obvious approach is to take $\ns$ samples $X_1,\dots,X_\ns$, count the number of pairs that show a collision, and use that as an unbiased estimator $Z_1$ for $\normtwo{\p}^2$:
\begin{equation}
  \label{eq:def:z1}
    Z_1 = \frac{1}{\binom{\ns}{2}} \sum_{1\leq s < t \leq \ns} \indic{X_s=X_t}\,.
\end{equation}
By the above, $\expect{Z_1} = \normtwo{\p}^2$. If we threshold $Z_1$ somewhere between (i) and (ii), at say $\tau\eqdef (1+2\dst^2)/\ab$, we should be able to distinguish between our two cases and get a valid tester. But how large must $\ns$ be for this to work? 

\begin{algorithm}[ht!]
  \begin{algorithmic}[1]
    \Require Multiset of $\ns$ samples $x_1,\dots,x_\ns \in \domain$, parameters $\dst\in(0,1]$ and $\ab = \abs{\domain}$
    \State Set $\tau \gets \frac{1+2\dst^2}{\ab}$
    \State Compute \Comment{Can be done in $O(\ns)$ time if $\domain$ is known, $O(\ns\log\ns)$ if only $\ab$ is.}
    \[
        Z_1 = \frac{1}{\binom{\ns}{2}} \sum_{1\leq s < t \leq \ns} \indic{x_s=x_t} = \frac{1}{\binom{\ns}{2}} \sum_{j\in\domain} \binom{\ns_j}{2}
    \] where $\ns_j \gets \sum_{t=1}^\ns\indic{x_t=j}$.
    \If{ $Z_1 \geq \tau$ } \Return \reject \Comment{Not uniform}
    \Else\ 
      \Return \accept \Comment{Uniform}
    \EndIf
  \end{algorithmic}
  \caption{\label{algo:collision-based}\sc Collision-Based Tester}
\end{algorithm}

Intuitively, we expect the test to work as long as the standard deviation of $Z_1$ (the ``noise'') is smaller than the gap between the expectations in our two cases (the ``signal''); that is,
\begin{equation}
  \label{eq:signal:to:noise}
      \sqrt{\Var[Z_1]} \ll \Delta \expect{Z_1} = \frac{4\dst^2}{\ab}
\end{equation}
as this is the condition for the random fluctuations of our statistic $Z_1$ not to ``cross'' our threshold too often and lead to a wrong answer.

To make this quantitative, we can use Chebyshev's inequality, which requires us to bound $\Var[Z_1]$. This is where things get tricky, since $Z_1$ is the sum of $\binom{\ns}{2}$ random variables which are \emph{not} pairwise independent.\footnote{Namely, the summands $\indic{X_s=X_t}$ in the definition of $Z_1$ are \emph{positively correlated}: $\cov(\indic{X_s=X_t},\indic{X_{s'}=X_{t'}}) \geq 0$, and are only independent if $s,s',t,t'$ are all distinct.} 

We first show how to derive a (suboptimal) bound $\ns=\bigO{\sqrt{\ab}/\dst^4}$:
\begin{align*}
  \Var[Z_1] 
   &= \bEE{Z_1^2} - \bEE{Z_1}^2\\
   &= \frac{1}{\binom{\ns}{2}^2} \sum_{1\leq s < t \leq \ns}\sum_{1\leq s' < t' \leq \ns} \bEE{\indic{X_s=X_t}\indic{X_{s'}=X_{t'}}} - \normtwo{\p}^4
\end{align*}
To handle this last sum despite the lack of independence of the summands, we will break it in 3 groups depending on the cardinality of $\{s,t,s',t'\}$, which can be either 4 (all indices are distinct), 3 (one index is common to the two pairs), or 2 (both pairs of indices are the same).
\begin{itemize}
  \item In the first case, we have independence of the two indicator random variables, and 
  \[
    \bEE{\indic{X_s=X_t}\indic{X_{s'}=X_{t'}}} = \bEE{\indic{X_s=X_t}}\bEE{\indic{X_{s'}=X_{t'}}} = \normtwo{\p}^4.
  \]
  \item In the third case, the two indicator random variables are the same, and since $\indic{}^2=\indic{}$ we get
  \[
    \bEE{\indic{X_s=X_t}\indic{X_{s'}=X_{t'}}} = \bEE{\indic{X_s=X_t}} = \normtwo{\p}^2.
  \]
  \item The second case is the messier one; still, one can verify that in this case $\indic{X_s=X_t}\indic{X_{s'}=X_{t'}}$ is 1 if, and only if, the three distinct samples corresponding to the 3 distinct indices among $s,t,s',t'$ take the same value, from which
  \[
    \bEE{\indic{X_s=X_t}\indic{X_{s'}=X_{t'}}} = \norm{\p}_3^3.
  \]
\end{itemize}
It remains to count how many summands of each type we have. Clearly, we have exactly $\binom{\ns}{2}$ summands of the third type; it is also not too hard to see that we have $\binom{\ns}{2}\binom{\ns-2}{2} = 6\binom{\ns}{4}$ summands of the first, and $6\binom{\ns}{3}$ of the second. (As a sanity check, $6\binom{\ns}{4}+6\binom{\ns}{3}+\binom{\ns}{2} = \binom{\ns}{2}^2$, so all our summands are accounted for.)

Getting back to our variance computation, this yields
\begin{align}
  \Var[Z_1] 
   &= \frac{1}{\binom{\ns}{2}^2} \Paren{ 6\binom{\ns}{4}\normtwo{\p}^4+6\binom{\ns}{3}\norm{\p}_3^3+\binom{\ns}{2}\normtwo{\p}^2 } - \normtwo{\p}^4 \notag\\
   &= \frac{1}{\binom{\ns}{2}^2} \Paren{ \Paren{6\binom{\ns}{4} - \binom{\ns}{2}^2}\normtwo{\p}^4+6\binom{\ns}{3}\norm{\p}_3^3+\binom{\ns}{2}\normtwo{\p}^2 } \label{eq:collisionbased:loose}\\
   &\leq \frac{4}{\ns}\norm{\p}_3^3+\frac{4}{\ns^2}\normtwo{\p}^2 \notag\\
   &\leq \frac{4}{\ns}\bEE{Z_1}^{3/2}+\frac{4}{\ns^2}\bEE{Z_1}  \notag
\end{align}
first using that $6\binom{\ns}{4} < \binom{\ns}{2}^2$ to discard a negative term, then that $\ns \geq 2$ to get a simpler-looking upper bound on binomial coefficients, and finally writing $\norm{\p}_3 \leq \normtwo{\p}$ by monotonicity of $\lp[p]$ norms.

In the uniform case (often called the \emph{completeness}\index{completeness} case for historical reasons), we seek to control the probability that $Z_1$ crosses our threshold $\tau \eqdef \frac{1+2\dst^2}{\ab}$, that is
\[
    \bPr{Z_1 \geq \tau} = \bPr{Z_1 \geq (1+2\dst^2)\bEE{Z_1}} \leq \bPr{Z_1 \geq (1+\dst^2)\bEE{Z_1}}
\]
while in the ``far'' case (often called the \emph{soundness}\index{soundness} case), we want to control
\[
    \bPr{Z_1 < \tau} \leq \bPr{Z_1 < \frac{(1-\dst^2)(1+4\dst^2)}{\ab}} \leq \bPr{Z_1 < (1-\dst^2)\bEE{Z_1}}
\]
using first that $(1-\dst^2)(1+4\dst^2) \geq 1+2\dst^2$ (for $\dst\leq 1/2$), and then the fact that in the ``far'' case $\bEE{Z_1} > \frac{1+4\dst^2}{\ab}$.

To control our probability of error in both cases, it is thus sufficient to upper bound $\bPr{\abs{Z_1-\bEE{Z_1}} \geq \dst^2\bEE{Z_1}}$; by Chebyshev's inequality (\cref{theo:chebyshev}), this is at most
\begin{align*}
    \bPr{\abs{Z_1-\bEE{Z_1}} \geq \dst^2\bEE{Z_1}}
    &\leq \frac{\Var[Z_1] }{\dst^4\bEE{Z_1}^2} \\
    &\leq \frac{4}{\dst^4\ns\bEE{Z_1}^{1/2}}+\frac{4}{\dst^4\ns^2\bEE{Z_1}} \\
    &\leq \frac{4\sqrt{\ab}}{\dst^4\ns}+\frac{4\ab}{\dst^4\ns^2}
\end{align*}
which is at most $1/3$, as desired, for $\ns \geq \frac{13\sqrt{\ab}}{\dst^4}$. (For the third inequality, we relied on the fact that $\bEE{Z_1} = \normtwo{\p}^2 \geq 1/\ab$ (\cf \cref{rk:collision:probability}).) \qed\medskip

The above argument establishes that the collision-based tester has sample complexity $O(\sqrt{\ab}/\dst^4)$. This is not the best one can get; in fact, by being (much) more careful one can show that this tester achieves the optimal sample complexity (as a function of $\ab$ and $\dst$)!
\begin{theorem}
The collision-based tester (\cref{algo:collision-based}) is a testing algorithm for uniformity with sample complexity $\ns(\ab,\dst,1/3) = O(\sqrt{\ab}/\dst^2)$ and time complexity $O(\ns)$.
\end{theorem}
\begin{proof}
  Clearly, if we are to prove this tighter bound, we have to be less heavy-handed in one of the steps of the previous analysis, specifically in bounding the variance. An obvious candidate is the first step featuring an inequality instead of an equality, just after~\cref{eq:collisionbased:loose}: there, we discarded a term since its coefficient $6\binom{\ns}{4} - \binom{\ns}{2}^2$ was negative.
  
  Maybe we should not have. Starting again at~\cref{eq:collisionbased:loose} and recalling the relation
  $
    \binom{\ns}{2}^2 = 6\binom{\ns}{4} + 6\binom{\ns}{3} + \binom{\ns}{2}
  $ we saw earlier, we have 
  \begin{align}
  \binom{\ns}{2}^2\Var[Z_1]
   &= \Paren{6\binom{\ns}{4} - \binom{\ns}{2}^2}\normtwo{\p}^4+6\binom{\ns}{3}\norm{\p}_3^3+\binom{\ns}{2}\normtwo{\p}^2 \notag\\
   &= \binom{\ns}{2}\normtwo{\p}^2(1-\normtwo{\p}^2) + 6\binom{\ns}{3}(\norm{\p}_3^3-\normtwo{\p}^4)
  \end{align}
  A detour: this quite interesting! The first term is exactly the variance we would get had we had independent summands (this is exactly the variance of a Binomial with parameters $\binom{\ns}{2}$ and $\normtwo{\p}^2$), while the second is the ``positive correlation'' term. Indeed, one can see that the second term is always nonegative, as
  \[
      \normtwo{\p}^4 %= \Paren{\sum_{i} \p(i)^2 }^2
      = \Paren{\sum_{i} \p(i)^{3/2} \p(i)^{1/2} }^2
      \leq \sum_{i} \p(i)^{3} \sum_{i}\p(i) = \norm{\p}_3^3
  \]
  by Cauchy--Schwarz (with equality if, and only if, $\p$ is uniform on its support). Going back to our variance analysis, we will simplify a little the RHS above for the sake of conciseness, leading to
  \begin{align}
    \label{eq:good:variance}
  \Var[Z_1]
   &\leq \frac{4}{\ns^2}\normtwo{\p}^2 + \frac{4}{\ns}(\norm{\p}_3^3-\normtwo{\p}^4)
  \end{align}
  at the cost of some small constant factors (and assuming without loss of generality that $\ns \geq 2$).
  
  \begin{itemize}
    \item In the uniform case, we have $\norm{\uniform_\ab}_3^3=\normtwo{\uniform_\ab}^4$ and~\cref{eq:good:variance} gives $\Var[Z_1] \leq \frac{4}{\ns^2\ab}$, and similarly as before
    \[
    \bPr{Z_1 \geq \tau} = \bPr{Z_1 \geq (1+2\dst^2)\bEE{Z_1}} \leq \frac{\Var[Z_1]}{4\dst^4\bEE{Z_1}^2}
    \leq \frac{\ab}{\dst^4\ns^2}
    \]
    by Chebyshev's inequality, and using $\bEE{Z_1}=1/\ab$. This in turn is less than $1/3$ as long as $\ns \geq \sqrt{3\ab}/\dst^2$.
    \item In the ``far'' case, we will be a little more careful. Let $\alpha^2 \eqdef \ab\normtwo{\p-\uniform_\ab}^2 \geq 4\dst^2$, so that $\bEE{Z_1} = \frac{1+\alpha^2}{\ab}$. The probability that our tester errs in this case is
    \begin{align*}
      \bPr{Z_1 < \tau} 
      &= \bPr{Z_1 < \frac{1+2\dst^2}{1+\alpha^2}\bEE{Z_1}} \\
      &= \bPr{Z_1 < \Paren{1-\frac{\alpha^2-2\dst^2}{1+\alpha^2}}\bEE{Z_1}} \\
      &\leq \bPr{Z_1 < \Paren{1-\frac{\alpha^2}{2(1+\alpha^2)}}\bEE{Z_1}} \\
      &\leq \frac{4(1+\alpha^2)^2}{\alpha^4}\cdot \frac{\Var[Z_1]}{\bEE{Z_1}^2} \\
      &\leq \frac{16(1+\alpha^2)^2}{\alpha^4\ns^2\normtwo{\p}^2} + \frac{16(1+\alpha^2)^2}{\alpha^4\ns}\cdot \frac{\norm{\p}_3^3-\normtwo{\p}^4}{\normtwo{\p}^4}
    \end{align*}
    the first inequality using $\alpha^2 \geq 4\dst^2$, the second being Chebyshev's, and the third being~\cref{eq:good:variance}. The first term is relatively familiar: since $\normtwo{\p}^2 = (1+\alpha^2)/\ab$ by definition of $\alpha$, we have
    \begin{equation}
      \label{eq:annoying:collision:soundness:firstterm}
        \frac{16(1+\alpha^2)^2}{\alpha^4\ns^2\normtwo{\p}^2} = \frac{16(1+\alpha^2)\ab}{\alpha^4\ns^2} \leq \frac{5\ab}{\dst^4\ns^2}
    \end{equation}
    the inequality since $x>0 \mapsto \frac{1+x}{x^2}$ is decreasing and $\alpha^2\geq 4\dst^2$ (and $\dst \leq 1$ to write $1+4\dst^2 \leq 5$). 
    
    To handle the second, we write $\p=(\p-\uniform_\ab)+\uniform_\ab$ and expand:%we use monotonicity of $x>0 \mapsto \frac{(1+x)^2}{x^2}$ to simplify a little the bound, then go on as follows:
    \begin{align*}
        \norm{\p}_3^3-\normtwo{\p}^4
        &\leq \norm{\p-\uniform_\ab+\uniform_\ab}_3^3-\frac{1}{\ab^2} \\
        &= \norm{\p-\uniform}_3^3 + \frac{3}{\ab}\normtwo{\p-\uniform}^2 \\
        &\leq \normtwo{\p-\uniform}^3 + \frac{3}{\ab}\normtwo{\p-\uniform}^2\\
        &= \frac{\alpha^{3}}{\ab^{3/2}} + \frac{3\alpha^2}{\ab^2}
    \end{align*}
    where the first inequality uses $\normtwo{\p}^2 \geq 1/\ab$, the equality follows from expanding the cubes in $\sum_i ((\p(i)-\uniform_\ab(i))+\uniform_\ab(i))^3$ and observing the cancellations, the second inequality is monotonicity of $\lp[p]$ norms, and the last equality stems from the definition of $\alpha$. This gives
    \begin{align}
        \frac{16(1+\alpha^2)^2}{\alpha^4\ns}\cdot \frac{\norm{\p}_3^3-\normtwo{\p}^4}{\normtwo{\p}^4}
        &= \frac{16\ab^2}{\alpha^4\ns}\cdot (\norm{\p}_3^3-\normtwo{\p}^4) \notag\\
        &\leq \frac{16\sqrt{\ab}}{\alpha\ns}+ \frac{48}{\alpha^2\ns} \notag\\
        &\leq \frac{16\sqrt{\ab}}{\alpha\ns}+ \frac{48}{\alpha^2\ns} \notag\\
        &\leq \frac{8\sqrt{\ab}}{\dst\ns}+ \frac{12}{\dst^2\ns}\,. \label{eq:annoying:collision:soundness:secondterm}
    \end{align}
    Combining~\cref{eq:annoying:collision:soundness:firstterm,eq:annoying:collision:soundness:secondterm}, we get
    \[
        \bPr{Z_1 < \tau} \leq \frac{5\ab}{\dst^4\ns^2} + \frac{8\sqrt{\ab}}{\dst\ns}+ \frac{12}{\dst^2\ns}
        \leq \frac{5\ab}{\dst^4\ns^2} + \frac{20\sqrt{\ab}}{\dst^2\ns}
    \]
    which is less than $1/3$ for $\ns \geq 64\sqrt{\ab}/\dst^2$.
  \end{itemize}
  Combining the uniform and far cases proves the theorem, showing that $\ns = O(\sqrt{\ab}/\dst^2)$ samples suffice for the collision-based tester to be correct with probability at least $2/3$ in both cases.
\end{proof}

%%to get the optimal bound $O(\sqrt{\ab}/\dst^2)$ instead of an (easier to obtain) $O(\sqrt{\ab}/\dst^4)$, the analysis of the variance has to be quite intricate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Unique elements} Another idea: count the number of elements that appear exactly \emph{once} among the $\ns$ samples taken. Why is that a sensible thing to do? We have seen that the uniform distribution will have the fewer collisions, so, equivalently, will have the maximum number of unique elements. In this case, the estimator $Z_2$ (the number of unique elements) is defined as
\begin{equation}
  \label{eq:def:z2}
    Z_2 = \frac{1}{\ns} \sum_{j\in\domain} \indic{\ns_j=1}\,,
\end{equation}
again with $\ns_j \eqdef \sum_{t=1}^\ns \indic{X_t=j}$. It is a simple matter to verify that this statistic has expectation\exercise{Do it!}
\begin{equation}
  \label{eq:z2:expectation}
  \expect{Z_2} = \sum_{i\in\domain} \p(i)(1-\p(i))^{\ns-1}
\end{equation}
which is\dots{} a thing? Note that under the uniform distribution $\uniform_\ab$, this is exactly $(1-1/\ab)^{\ns-1}\approx 1 - \frac{\ns}{\ab}$, and under arbitrary $\p$ this is (making a few approximations not always valid) $\approx \sum_{i=1}^\ab \p(i)(1-\ns\p(i)) = 1 - \ns\normtwo{\p}^2$. So the gap in expectation between the two cases ``should'' be around $4\dst^2\ns/\ab$, and, if the variance analysis goes well and the stars align, we will be able to use Chebyshev's inequality and argue that we can distinguish the two for $\ns$ large enough.

\begin{algorithm}[ht!]
  \begin{algorithmic}[1]
    \Require Multiset of $\ns$ samples $x_1,\dots,x_\ns \in \domain$, parameters $\dst\in(0,1]$ and $\ab = \abs{\domain}$ such that $\dst \geq \frac{15}{\ab^{1/4}}$
    \State Set $\tau \gets (1-\frac{1}{\ab})^{\ns-1}-\frac{\ns\dst^2}{8\ab}$ \Comment{This is $\bE{\uniform_\ab}{Z_2} - \frac{\ns\dst^2}{8\ab}$}
    \State Compute \Comment{Can be done in $O(\ns)$ time if $\domain$ is known, $O(\ns\log\ns)$ if only $\ab$ is.}
    \[
        Z_2 = \frac{1}{\ns} \sum_{j\in\domain} \indic{\ns_j=1}
    \] where $\ns_j \gets \sum_{t=1}^\ns\indic{x_t=j}$.
    \If{ $Z_2 \leq \tau$ } \Return \reject \Comment{Not uniform}
    \Else\ 
      \Return \accept \Comment{Uniform}
    \EndIf
  \end{algorithmic}
  \caption{\label{algo:unique-elements}\sc Unique-Elements Tester}
\end{algorithm}

Now, before we actually delve into this analysis, it is worth mentioning a limitation of this tester, which is that we only expect it to work for $\ns \ll \ab$. Indeed, we count the number of \emph{distinct} elements, and there will never ever be more than $\ab$ of them if the domain size is $\ab$.\footnote{More quantitatively, for $\ns \to \infty$ the approximations made in the above discussion completely break down; and we will instead get $\expect{Z_2} \to 0$ in both the uniform and the far cases.} That explains, intuitively, the condition for the test to work: we need $\ns$ (the number of samples taken) to be smaller than $\ab$ (the maximum number of distinct elements one can ever hope to see), which gives, since we will eventually get $\ns = \Theta(\sqrt{\ab}/\dst^2)$, the condition $\dst \gg 1/\ab^{1/4}$.

Our first step is to make our back-of-the-envelope computation above rigorous, and lower bound the gap $\Delta(\p) \eqdef \bE{\uniform_\ab}{Z_2} - \bE{\p}{Z_2}$ between the far and uniform cases.
\begin{lemma}
  \label{lemma:gap:z2}
If $\ns \leq \ab$, we have
\[
    \bE{\uniform_\ab}{Z_2} - \bE{\p}{Z_2} \geq \frac{\ns}{16\ab}\totalvardist{\p}{\uniform_\ab}^2\,.
\]
\end{lemma}
\begin{proof}
Denote this gap by $\Delta(\p)$. 
From~\eqref{eq:z2:expectation}, we can explicitly write
\begin{align*}
  \Delta(\p) 
  &= \Paren{1-1/\ab}^{\ns-1} - \sum_{i\in\domain} \p(i)(1-\p(i))^{\ns-1} \\
  &= \sum_{i\in\domain} \p(i)\Paren{ \Paren{1-1/\ab}^{\ns-1} - (1-\p(i))^{\ns-1} } \\
  &= \Paren{1-1/\ab}^{\ns-1}\sum_{i\in\domain} \p(i)\Paren{ 1 - \Paren{\frac{1-\p(i)}{1-1/\ab}}^{\ns-1} }
\end{align*}
where in the second line we used $\sum_i \p(i)=1$ to ``hide $1$.'' Defining $f\colon[0,1]\to\R$ by 
\[
  f(x) = x\Paren{ 1 - \Paren{\frac{1-x}{1-1/\ab}}^{\ns-1} }
\] and using the fact that $\ns \leq \ab$  to write $(1-1/\ab)^{\ns-1} \geq (1-1/\ab)^{\ab} \geq 1/4$ (as $\ab \geq 2$), we are left with
$
  \Delta(\p) \geq \frac{1}{4}\sum_{i\in\domain} f(\p(i))
$. At this point, we would like to rely on tools from our arsenal (convexity or concavity for Jensen's inequality, monotonicity, etc.); unfortunately, the function $f$ is not very well-behaved, and is neither concave, convex, or monotone. It does satisfy $f(0)=f(1/\ab)=0$, $f(1)=1$, but that is not quite enough. Instead, we will find a ``good'' lower bound $g$ on $f$, which will allow us to reason more easily: specifically, we will set, for $x\in[0,1]$,
\[
    g(x) = \frac{\ns-1}{\ab-1}\cdot (x-1/\ab) + \frac{\ns-1}{2(1-1/\ab)}(x-1/\ab)^2\indic{x\leq 1/\ab}\,.
\]
Let us demystify this choice a little. The first coefficient is simply $f'\Paren{1/\ab}$, while the second has been chosen as the largest possible value such that $g'(0)\geq 0$.\footnote{While we will not require it, this ensures $g$ is nondecreasing, which is nice.} Moreover, when summing $\sum_i g(\p(i))$, the linear term will just cancel out and we will be left with a quadratic term of the form $\sum_i g(\p(i)) \asymp \sum_i (\p(i)-1/\ab)^2\indic{\p(i)\leq 1/\ab}$, which we can hope to relate to $\normtwo{\p-\uniform_\ab}^2$ (the same thing, without the indicator). An illustration of $f$ and $g$ is given in~\cref{fig:z2:fg}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]\centering
  \includegraphics[width=1.0\textwidth]{figures/fig-uniformity-z2-fg}
  \caption{\label{fig:z2:fg}Our choice of $g$, here depicted for $\ab=8$, $\ns=7$.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To continue our analysis, we will rely on the following technical claim, whose proof is calculus and left to the reader.\exercise{You are the reader.}
\begin{claim}
Fix $\alpha\in(0,1)$ and $\beta \geq 1$ such that $\frac{\alpha\beta}{1-\alpha} \leq 1$; and define $f_{\alpha,\beta},g_{\alpha,\beta}\colon [0,1]\to\R$ by
$
f_{\alpha,\beta}(x) = x\Paren{1-\Paren{\frac{1-x}{1-\alpha}}^\beta}$ and 
\[
g_{\alpha,\beta}(x) = \frac{\alpha\beta}{1-\alpha}(x-\alpha) + \frac{\beta}{2(1-\alpha)} (x-\alpha)^2 \indic{x\leq \alpha}\,.
\]
Then $g_{\alpha,\beta}$ is nondecreasing, and $f_{\alpha,\beta}\geq g_{\alpha,\beta}$.
\end{claim}
Applying this to $\alpha\eqdef 1/\ab$ and $\beta \eqdef \ns-1$ (which, since $\ns\leq \ab$, satisfy the assumptions) leads to
\begin{align*}
  \Delta(\p) 
  &%\geq \frac{1}{4}\sum_{i\in\domain} f(\p(i)) 
  \geq \frac{1}{4}\sum_{i\in\domain} g(\p(i)) \\
  &= \frac{\ns-1}{8(1-1/\ab)} \sum_{i\in\domain} (\p(i)-1/\ab)^2\indic{\p(i)\leq 1/\ab} \\
  &\geq \frac{\ns-1}{8(\ab-1)} \Paren{\sum_{i\in\domain} \abs{\p(i)-1/\ab}\indic{\p(i)\leq 1/\ab}}^2 \\
  &= \frac{\ns-1}{8(\ab-1)} \totalvardist{\p}{\uniform_\ab}^2\,,
\end{align*}
where the first line uses $f\geq g$, the second from cancellation of the linear term of $g$, the third is Cauchy--Schwarz, and the last follows from the definition of total variation distance. Using $\ns \geq 2$ to write $\frac{\ns-1}{\ab-1}\geq \frac{\ns}{2\ab}$ concludes the proof.
\end{proof}
At this point, we have half of the puzzle: it remains to get a handle on the variance of $Z_2$ in both the far and uniform cases. We start by providing an exact (albeit unwieldy) expression for it.%, which we will then massage to obtain a usable bound in the uniform case.
\begin{align*}
\Var[Z_2] 
&= \frac{1}{\ns^2}\sum_{i,j\in\domain} \bEE{\indic{\ns_i=1}\indic{\ns_j=1}} - \bEE{Z_2}^2  \\
&= \frac{1}{\ns^2}\sum_{i\in\domain} \bEE{\indic{\ns_i=1}} + \frac{1}{\ns^2}\sum_{i\neq j} \bEE{\indic{\ns_i=1}\indic{\ns_j=1}} - \bEE{Z_2}^2 \\
&= \frac{1}{\ns}\bEE{Z_2}- \bEE{Z_2}^2 + \frac{\ns-1}{\ns}\sum_{i\neq j} \p(i)\p(j)(1-(\p(i)+\p(j)))^{\ns-2}\,, %\label{eq:var:z2}
\end{align*}
where the last line relied on~\cref{eq:def:z2} to recognize $\bEE{Z_2}$ in the first term, and on the fact that $\bEE{\indic{\ns_i=1}\indic{\ns_j=1}} = \ns(\ns-1)\p(i)\p(j)(1-(\p(i)+\p(j)))^{\ns-2}$ for $i\neq j$. (Indeed, $\indic{\ns_i=1}\indic{\ns_j=1}$ is equal to one if, and only if, out of the $\ns$ samples two fall on $i$ and $j$, respectively, and the $\ns-2$ others hit $\domain\setminus\{i,j\}$.)

This looks quite unwieldy; however, we can rearrange the term $\bEE{Z_2}^2$ to obtain the nicer expression
\begin{align}
\Var[Z_2] 
&= \frac{\bEE{Z_2}(1-\bEE{Z_2})}{\ns} + \frac{\ns-1}{\ns}\Paren{\sum_{i\neq j} \p(i)\p(j)(1-\p(i)-\p(j))^{\ns-2} - \bEE{Z_2}^2 } \notag\\
&\leq \frac{1-\bEE{Z_2}}{\ns} + \sum_{i\neq j} \p(i)\p(j)(1-\p(i)-\p(j))^{\ns-2} - \bEE{Z_2}^2 \label{eq:var:z2}
\end{align}
For the uniform case,~\cref{eq:var:z2} simplifies quite a bit, and we get
\begin{align}
\Var_{\uniform_{\ab}}[Z_2] 
%&\leq \frac{1}{\ns} \Paren{ 1-\Paren{1-\frac{1}{\ab}}^{\ns-1} } + \frac{\ab-1}{\ab}\Paren{1-\frac{2}{\ab}}^{\ns-2} - \Paren{1-\frac{1}{\ab}}^{2(\ns-1)} \notag\\
&\leq \frac{1}{\ns} \Paren{ 1-\Paren{1-\frac{1}{\ab}}^{\ns-1} } + \Paren{1-\frac{2}{\ab}}^{\ns-2} - \Paren{1-\frac{1}{\ab}}^{2(\ns-1)} \notag\\
&\leq \frac{\ns-1}{\ns\ab} + \Paren{1-\frac{1}{\ab}}^{2(\ns-2)} - \Paren{1-\frac{1}{\ab}}^{2(\ns-1)} \notag\\
&\leq \frac{1}{\ab} + \Paren{1-\frac{1}{\ab}}^{2(\ns-2)} \Paren{ 1 - \Paren{1-\frac{1}{\ab}}^{2} } \notag\\
&= \frac{1}{\ab} + \frac{2}{\ab}\Paren{1-\frac{1}{\ab}}^{2(\ns-2)} \Paren{ 1-\frac{1}{2\ab} } \leq \frac{3}{\ab}
\label{eq:var:z2:unif}
\end{align}
where the second inequality follows from $1-2x \leq (1-x)^2$, and $\Paren{1-x}^{m} \geq 1-mx$ for $m\geq 1$ and $x \leq 1$. (As a side note, we proved along the way that $\frac{1}{\ns}\Paren{1-\bE{\uniform_{\ab}}{Z_2}} \leq \frac{1}{\ab}$, which will come in handy.)

This looks great! We just proved that, at least in the uniform case, $\Var_{\uniform_{\ab}}[Z_2] \leq 3/\ab$. By the same rule of thumb as in the previous argument (\cref{eq:signal:to:noise}), we expect our test to work as long as the standard deviation (the ``noise'') of our statistic is smaller than the gap in expectations (the ``signal''), which by~\cref{lemma:gap:z2} gives the condition
\[
   \Var_{\uniform_{\ab}}[Z_2] \leq \frac{3}{\ab} \ll \frac{\dst^4\ns^2}{16\ab^2} \leq \Delta(\p)^2\,.
\]
Reorganizing, this yields the condition $\ns \gg \sqrt{\ab}/\dst^2$, which is exactly what we want to prove. The problem, of course, is that we so far only have bounded the variance in one of the two cases; to conclude, we need the last quarter of the puzzle.

To do so, we will invoke the following:
\begin{lemma}
  \label{lemma:technical:distinct:elements}
Fix $m \geq 1$ and $\ab \in \N$. For any $x_1,\dots,x_\ab \geq 0$ such that $\sum_{i=1}^\ab x_i = 1$, we have
\[
    \frac{m\sum_{1\leq i < j \leq \ab} x_i x_j \Paren{ (1-x_i - x_j)^{m-1} - (1-x_i)^m(1-x_j)^m} }{\sum_{i=1}^\ab x_i (1-(1-x_i)^m)} \leq 1
\]
\end{lemma}
Unfortunately, there is not much intuition we can provide about \emph{why} this inequality holds; and we defer its proof to the end of the chapter (\cref{sec:deferred:chap:identity}), focusing for now on how it will provide us with the last piece of said puzzle. In view of resuming from~\cref{eq:var:z2}, we expand $\bEE{Z_2}^2$ to write
\begin{align*}
\sum_{i\neq j} &\p(i)\p(j)(1-\p(i)-\p(j))^{\ns-2} - \bEE{Z_2}^2  \\
&= \sum_{i\neq j} \p(i)\p(j)(1-\p(i)-\p(j))^{\ns-2} - \sum_{i,j} \p(i)\p(j)(1-\p(i))^{\ns-1}(1-\p(j))^{\ns-1} \\
&\leq \sum_{i\neq j} \p(i)\p(j)\Paren{ (1-\p(i)-\p(j))^{\ns-2} - (1-\p(i))^{\ns-1}(1-\p(j))^{\ns-1} } \\
&\leq \frac{1}{\ns-1} \sum_{i=1}^\ab \p(i) (1-(1-\p(i))^{\ns-1}) = \frac{1-\bEE{Z_2}}{\ns-1}
\end{align*}
where the last inequality is~\cref{lemma:technical:distinct:elements} applied with $m=\ns-1$ and $x_i = \p(i)$. Then, using this in~~\cref{eq:var:z2} (and bounding $1/(\ns-1) \leq 2/\ns$) leads to 
\begin{align}
\Var[Z_2]
&\leq 3\cdot \frac{1-\bE{\p}{Z_2}}{\ns} 
= 3\Paren{ \frac{1-\bE{\uniform_{\ab}}{Z_2}}{\ns} + \frac{\bE{\uniform_{\ab}}{Z_2}-\bE{\p}{Z_2}}{\ns} }  \notag\\
&\leq 3\Paren{ \frac{1}{\ab} + \frac{\Delta(\p)}{\ns} }\,. \label{eq:var:z2:final}
\end{align}
Before invoking Chebyshev's inequality, let us see why this is wonderful news. The first term of the bound, $3/\ab$, is the same as in the uniform case, and we have discussed before how it would by itself lead to the desired sufficient condition $\ns \gg \sqrt{\ab}/\dst^2$. The second is new; by the same rule of thumb, it will lead to the condition
\[
    \frac{3\Delta(\p)}{\ns} \ll \Delta(\p)^2
\]
where one $\Delta(\p)$ crucially cancels out, leaving us with $\Delta(\p) \gg 1/\ns$. Since $\Delta(\p)\geq \frac{\dst^2\ns}{16\ab}$, a sufficient condition then becomes $\frac{\dst^2\ns}{\ab} \gg \frac{1}{\ns}$, which then will be satisfied as soon as $\ns \gg \sqrt{\ab}/\dst$.

Now that we have all the pieces of the puzzle, let us establish the main result of this subsection:
\begin{theorem}
  \label{theo:uniformity:unique:elements}
The unique-elements tester (\cref{algo:unique-elements}) is a testing algorithm for uniformity with sample complexity $\ns(\ab,\dst,1/3) = O(\sqrt{\ab}/\dst^2)$ and time complexity $O(\ns)$, provided that $\dst \geq 15/\ab^{1/4}$.
\end{theorem}
\begin{proof}
For any $\p\in\distribs{\ab}$, let as before $\Delta(\p) \eqdef \bE{\uniform_\ab}{Z_2} - \bE{\p}{Z_2}$. Of course, if $\p=\uniform_\ab$ then $\Delta(\p)=0$, and we know from~\cref{lemma:gap:z2} that $\Delta(\p) \geq \Delta \eqdef \frac{\ns\dst^2}{16\ab}$ whenever $\totalvardist{\p}{\uniform_\ab} \geq \dst$.\footnote{We assume throughout $\ns \leq \ab$, and will enforce this at the end.} We also obtained earlier (in~\cref{eq:var:z2:unif,eq:var:z2:final}) a bound in the variance of $Z_2$ in both the uniform and ``far'' cases, so we have all the ingredients we need. Define our threshold
\[
    \tau \eqdef \bE{\uniform_\ab}{Z_2} - \frac{\Delta}{2}
\]
as in~\cref{algo:unique-elements}.
\begin{itemize}
    \item In the uniform case, the probability to output \reject (and thus make a mistake) is bounded as
    \[
    \bPr{Z_2 \leq \tau} = \bPr{Z_2 \leq \bE{\uniform_\ab}{Z_2} - \frac{\Delta}{2} } \leq \frac{4\Var_{\uniform_\ab}[Z_2]}{\Delta^2}
    \leq \frac{3072\ab}{\dst^4\ns^2}
    \]
    by Chebyshev's inequality, using $\Var_{\uniform_\ab}[Z_2] \leq 3/\ab$ and the definition of $\Delta$. This in turn is less than $1/3$ as long as $\ns \geq 96\sqrt{\ab}/\dst^2$.
    \item In the ``far'' case, since $\bE{\p}{Z_2} = \bE{\uniform_\ab}{Z_2} - \Delta(\p)$ and $\Delta(\p) \geq \Delta$ the probability to err by outputting \accept is
    \begin{align*}
        \bPr{Z_2 > \tau} &= \bPr{Z_2 > \bE{\p}{Z_2} + \frac{\Delta(\p)}{2} + \frac{\Delta(\p)-\Delta}{2} } \\       
        &\leq \bPr{Z_2 > \bE{\p}{Z_2} + \frac{\Delta(\p)}{2} } \\
        &\leq \frac{4\Var_{\p}[Z_2]}{\Delta(\p)^2} 
        \leq \frac{12}{\ab \Delta(\p)^2} + \frac{12}{\ns \Delta(\p)} \\
        &\leq \frac{3072\ab}{\dst^4\ns^2} + \frac{192\ab}{\dst^2\ns^2} \leq \frac{3264\ab}{\dst^4\ns^2}
    \end{align*}
    using $\Var_{\p}[Z_2] \leq 3/\ab+3\Delta(\p)/\ns$. The resulting bound is then less than $1/3$ for $\ns \geq 99\sqrt{\ab}/\dst^2$.
  \end{itemize}
  The above analysis shows that both errors are less than $1/3$ for, say, $\ns = \lceil{99\sqrt{\ab}/\dst^2}\rceil$. However, we did rely on~\cref{lemma:gap:z2}, which requires $\ns \leq \ab$; given our choice of $\ns$, this in turns imposes a condition on $\dst$ (for instance, one can check that $\dst \geq 15/\ab^{1/4}$ suffices).
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modified $\chi^2$} If you are a statistician, or just took a Statistics class, or even got lost on Wikipedia at some point and ended up on the wrong page at the wrong time, you may know of Pearson's $\chi^2$ test for goodness-of-fit: for every element $i$ of the domain, count how many times it appeared in the samples, $N_i$. Compute $\sum_{i} \frac{(N_i-\ns/\ab)^2}{\ns/\ab}$. Compare the result to a predetermined threshold. This very natural idea, maybe not surprisingly, works well! In particular, since $N_i \sim \binomial{\ns}{\p(i)}$, it is not hard to see that
\[
\bEE{ \sum_{i=1}^\ab \frac{(N_i-\ns/\ab)^2}{\ns/\ab} } = \frac{\ab}{\ns}\sum_{i=1}^\ab \bEE{ \Paren{N_i-\frac{\ns}{\ab}}^2 } = \ab(\ns-1)\normtwo{\p-\uniform_\ab}^2 + \ab-1
\]
using moments of a Binomial random variable and~\cref{eq:relation:collisionprob:distance:uniform}.\exercise{Check it!} This does look like a reasonable way to estimate the distance to uniformity \emph{via} the $\lp[2]$ distance again\dots{} unfortunately, the variance will be slightly annoying, due to the correlated between terms of the sums.


To make the task easier, it is helpful to think of taking $\poisson{\ns}$ samples instead of exactly $\ns$, which will greatly simplifies the analysis. Then, under this (slightly different) sampling model the $N_i$'s become independent, with $N_i\sim\poisson{\ns\p(i)}$: this is called \emph{Poissonization}\index{Poissonization}, and can be done more or less without loss of generality since a $\poisson{\ns}$ random variable will be between $0.99\ns$ and $1.01\ns$ with overwhelming probability. (See~\cref{sec:poissonization} for more on Poissonization, and why we can use it ``without loss of generality'').

\begin{algorithm}[ht!]
  \begin{algorithmic}[1]
    \Require Multiset of $\ns$ samples $x_1,\dots,x_\ns \in \domain$, parameters $\dst\in(0,1]$ and $\ab = \abs{\domain}$
    \Comment{Assumes Poissonization}
    \State Set $\tau \gets 2\ns\dst^2$
    \State Compute \Comment{Can be done in $O(\ns)$ time if $\domain$ is known, $O(\ns\log\ns)$ if only $\ab$ is.}
    \[
        Z_3 = \sum_{j\in\domain} \frac{(\ns_j-\ns/\ab)^2-\ns_j}{\ns/\ab}
    \] where $\ns_j \gets \sum_{t=1}^\ns\indic{x_t=j}$.
    \If{ $Z_3 \geq \tau$ } \Return \reject \Comment{Not uniform}
    \Else\ 
      \Return \accept \Comment{Uniform}
    \EndIf
  \end{algorithmic}
  \caption{\label{algo:chisquare}\sc Chi-Square Tester}
\end{algorithm}

The bad news is that it does not actually lead to the optimal sample complexity: Poissonization introduces a bit more variance (as we introduce extra randomness ourselves by taking a random number of samples), and so the variance of this $\chi^2$ test can be too big due to the elements we only expect to see zero or once (so, most of them). The \emph{good} news is that a simple correction of that test, of the form
\begin{equation}
    Z_3 = \sum_{i=1}^\ab \frac{(N_i-\ns/\ab)^2- N_i}{\ns/\ab}
\end{equation}
\emph{does} have a much smaller variance, and a threshold test of the form ``$Z_3 > \tau$?'' will yield the right sample complexity.\footnote{In the multinomial (``non-Poissonized'') case, substracting $N_i$ was not necessary, since that would correspond to removing overall $\frac{\ab}{\ns}\sum_{i=1}^\ab N_i = \ab$, a constant term. In the Poissonized case, however, $\sum_{i=1}^\ab N_i\sim\poisson{\ns}$, and $\frac{\ab}{\ns}\sum_{i=1}^\ab N_i$ is not a constant -- it is a random variable with expectation $\ab$ and (large) variance $\ab^2/\ns$.} Recalling that $N_i\sim\poisson{\ns\p(i)}$ for all $i$, the expectation of $Z_3$ will then just be 
\begin{equation}
  \label{eq:expectation:z3}
    \expect{Z_3} = \ns\ab \normtwo{\p-\uniform_\ab}^2
\end{equation}
which is perfect. Analyzing this test boils down, again, to bounding the variance of $Z_3$ and invoking Chebyshev's inequality\dots{} Before doing so, we will make an innocuous change, but which will come quite handy in~\cref{sec:identity} when generalizing beyond the uniform distribution: let us rewrite
\[
    Z_3 = \sum_{i=1}^\ab \frac{(N_i-\ns\uniform_\ab(i))^2- N_i}{\ns\uniform_\ab(i)}
\]
where, of course, $\uniform_\ab(i) = 1/\ab$ for all $i\in[\ab]$. Recalling that, by Poissonization, all the $N_i$'s are independent Poisson random variables, we will invoke the following two technical claims, which follow from (somewhat tedious, but straightforward) computations involving the moments of Poisson random variables:\exercise{Prove this claim!}
\begin{claim}
  Let $\mu, \lambda \geq 0$. If $X\sim\poisson{\lambda}$, then 
  $
  \bEE{(X-\mu)^2-X} = (\lambda-\mu)^2
  $
  and 
  $
  \bEE{((X-\mu)^2-X)^2} = (\lambda-\mu)^4 + 2\lambda^2 + 4\lambda(\lambda-\mu)^2
  $.
\end{claim}
Given this, by linearity of expectation we immediately get
\begin{align}
    \bEE{Z_3} 
    &= \sum_{i=1}^\ab \frac{\bEE{(N_i-\ns\uniform_\ab(i))^2- N_i}}{\ns\uniform_\ab(i)}
    = \ns\sum_{i=1}^\ab \frac{(\p(i)-\uniform_\ab(i))^2}{\uniform_\ab(i)} \notag\\
    &= \ns\cdot\chisquare{\p}{\uniform_\ab} \label{eq:expectation:z3:chisquare}
\end{align}
which here can further be simplified by $\chisquare{\p}{\uniform_\ab} = \ab\normtwo{\p-\uniform_\ab}^2$, as the denominator is constant and equal to $1/\ab$; this establishes~\cref{eq:expectation:z3}.

Turning to the variance, we want to relate $\Var[Z_3]$ to known quantities, and in particular $\bEE{Z_3}$. To do so, we use independence of the $N_i$'s followed by the second part of the above claim to get
\begin{align}
    \Var[Z_3]
    &= \sum_{i=1}^\ab \frac{\Var[(N_i-\ns\uniform_\ab(i))^2- N_i]}{\ns^2\uniform_\ab(i)^2} \notag\\
    &= \sum_{i=1}^\ab \frac{2\ns^2\p(i)^2 + 4\ns^3\p(i)(\p(i)-\uniform_\ab(i))^2}{\ns^2\uniform_\ab(i)^2} \notag\\
    &= 2\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}+4\ns\sum_{i=1}^\ab \frac{\p(i)(\p(i)-\uniform_\ab(i))^2}{\uniform_\ab(i)^2}\notag\\
    &\leq 2\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}+4\ns\sqrt{\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}}\cdot\sqrt{\sum_{i=1}^\ab \frac{(\p(i)-\uniform_\ab(i))^4}{\uniform_\ab(i)^2}}\notag\\
    &\leq 2\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}+4\ns\sqrt{\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}}\cdot\sum_{i=1}^\ab \frac{(\p(i)-\uniform_\ab(i))^2}{\uniform_\ab(i)}\notag\\
    &= 2\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2} + 4\sqrt{\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}}\bEE{Z_3} \label{eq:z3:var:interm}\,,
\end{align}
where the first inequality is Cauchy--Schwarz, and the second is monotonicity of $\lp[p]$ norms: namely, $\lp[2] \leq \lp[1]$. In order to proceed further, we need to bound the quantity $\sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}$. The trick here will be to write, using $(a+b)^2 \leq 2a^2+2b^2$, 
\[
    \p(i)^2 = ((\p(i)-\uniform_\ab(i))+\uniform_\ab(i))^2 \leq 2(\p(i)-\uniform_\ab(i))^2 + 2\uniform_\ab(i)^2\,,
\]
since then we have
\begin{align*}
    \sum_{i=1}^\ab \frac{\p(i)^2}{\uniform_\ab(i)^2}
    &\leq 2\ab + 2\sum_{i=1}^\ab \frac{(\p(i)-\uniform_\ab(i))^2}{\uniform_\ab(i)^2}  
    = 2\ab + 2\ab\sum_{i=1}^\ab \frac{(\p(i)-\uniform_\ab(i))^2}{\uniform_\ab(i)} \\
    &= 2\ab\Paren{ 1 + \frac{\bEE{Z_3}}{\ns}  }\,.
\end{align*}
Putting this back in~\cref{eq:z3:var:interm}, we get
\begin{equation}
    \Var[Z_3]
    \leq 4\ab\Paren{ 1 + \frac{\bEE{Z_3}}{\ns}  } + 4\sqrt{2}\ab^{1/2}\bEE{Z_3}+4\sqrt{2}\frac{\ab^{1/2}}{\ns^{1/2}} \bEE{Z_3}^{3/2} \label{eq:z3:var}
\end{equation}
In particular, in the uniform case $\bE{\uniform_\ab}{Z_3} = 0$, and so $\Var_{\uniform_\ab}[Z_3] \leq 4\ab$. Before formally analyzing the resulting sample complexity via (once more) Chebyshev's inequality, let us do the usual check and compare standard deviation (noise) to expected gap (signal), and see if things look promising. The gap in expectation will be, given~\cref{eq:expectation:z3:chisquare}, at least $\Delta \eqdef \ns\ab \cdot \frac{4\dst^2}{\ab} = 4\ns\dst^2$; so, in the uniform case, we need
$\Var_{\uniform_\ab}[Z_3] \ll \Delta^2$ which, given the above, is satisfied as long as $\ns \gg \sqrt{\ab}/{\dst^2}$, since then
\[
    \Var_{\uniform_\ab}[Z_3] \leq 2\ab \ll 16\ns^2\dst^4 \leq \Delta^2\,.
\]
In the ``far'' case, for $\p$ at total variation distance at least $\dst$ from uniform, the condition $\Var_\p[Z_3] \ll \Delta(\p)^2 = \bE{\p}{Z_3}^2$, which by~\cref{eq:z3:var} will require
\[
    \max\Paren{\ab, \frac{\ab}{\ns}\bE{\p}{Z_3}, \ab^{1/2}\bE{\p}{Z_3} , \frac{\ab^{1/2}}{\ns^{1/2}}\bE{\p}{Z_3}^{3/2} } \ll \bE{\p}{Z_3}^2\,.
\]
Considering each term separately, simplifying, and recalling that $\bE{\p}{Z_3} \geq 4\ns\dst^2$, we see that this will also hold as long as $\ns \gg \sqrt{\ab}/{\dst^2}$.

We will make this formal, and show the following:
\begin{theorem}
The $\chi^2$-based tester (\cref{algo:chisquare}) is a testing algorithm for uniformity with sample complexity $\ns(\ab,\dst,1/3) = O(\sqrt{\ab}/\dst^2)$ and time complexity $O(\ns)$ in the Poissonized setting.
\end{theorem}
\begin{proof}
For any $\p\in\distribs{\ab}$, let as before $\Delta(\p) \eqdef \bE{\p}{Z_3}$. By~\cref{eq:expectation:z3}, we know that if $\p=\uniform_\ab$ then $\Delta(\p)=0$, and that $\Delta(\p) \geq \Delta \eqdef 4\ns\dst^2$ whenever $\totalvardist{\p}{\uniform_\ab} \geq \dst$ (recalling~\cref{eq:relation:l1:l2:cs}). We also have our variance bound from~\cref{eq:z3:var}. Define our threshold
\[
    \tau \eqdef \frac{\Delta}{2}
\]
as in~\cref{algo:chisquare}.
\begin{itemize}
    \item In the uniform case, where $\bE{\uniform_\ab}{Z_3}=0$, the probability to output \reject (and thus make a mistake) is bounded as
    \[
    \bPr{Z_3 \geq \tau} \leq \frac{4\Var_{\uniform_\ab}[Z_3]}{\Delta^2}
    \leq \frac{\ab}{\dst^4\ns^2}
    \]
    by Chebyshev's inequality, using $\Var_{\uniform_\ab}[Z_3] \leq 4\ab$ and the definition of $\Delta$. This in turn is less than $1/3$ as long as $\ns \geq \sqrt{3\ab}/\dst^2$.
    \item In the ``far'' case, since $\bE{\p}{Z_3} \geq \Delta$ the probability to err by outputting \accept is
    \begin{align*}
        \bPr{Z_3 < \tau} %&= \bPr{Z_3 < \frac{\Delta(\p)}{2}  } \\       
        &\leq \bPr{\abs{Z_3 - \bE{\p}{Z_3}} > \frac{\Delta(\p)}{2} } \\
        &\leq \frac{4\Var_{\p}[Z_3]}{\Delta(\p)^2} \\
        &\leq \frac{16\ab\Paren{ 1 + \frac{\bEE{Z_3}}{\ns}  } + 16\sqrt{2}\ab^{1/2}\bEE{Z_3}+16\sqrt{2}\frac{\ab^{1/2}}{\ns^{1/2}} \bEE{Z_3}^{3/2}}{\bEE{Z_3}^2} \\
        %&= \frac{16\ab}{\bEE{Z_3}^2} + \frac{16\ab}{\ns\bEE{Z_3}^{\vphantom{2}}} + \frac{16\sqrt{2}\ab^{1/2}}{\bEE{Z_3}^{\vphantom{2}}}+\frac{16\sqrt{2}\ab^{1/2}}{\ns^{1/2}\bEE{Z_3}^{1/2}} \\
        &\leq \frac{\ab}{\ns^2\dst^4} + \frac{4\ab}{\ns^2\dst^2} + \frac{4\sqrt{2}\ab^{1/2}}{\ns\dst^2}+\frac{8\sqrt{2}\ab^{1/2}}{\ns\dst} \\
        &\leq \frac{5\ab}{\ns^2\dst^4} + \frac{12\sqrt{2}\ab^{1/2}}{\ns\dst^2}
    \end{align*}
    first using~\cref{eq:z3:var}, simplifying, then $\bEE{Z_3}\geq \ns\dst^2$. By solving the inequality $5x^2 + 12\sqrt{2}x \leq 1/3$, we see that the result is at most $1/3$ for $\ns \geq 52\sqrt{\ab}/\dst^2$.
  \end{itemize}
  The above analysis shows that both errors are less than $1/3$ for, say, $\ns = \lceil{52\sqrt{\ab}/\dst^2}\rceil$; this concludes the proof.
\end{proof}

% It's a good exercise, and under the Poissonization assumption not that hard. (Try \emph{without} removing $N_i$ in the numerator, though, and see what you get\dots)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Empirical distance to uniform} Let us take a break from $\lp[2]$ and consider another, very natural thing to try: the \emph{plugin estimator}. Since we have $\ns$ samples from $\p$, we can compute the empirical estimator of the distribution, $\hat{\p}_\ns$, based on these $\ns$ samples. Now, we want to test $\totalvardist{\p}{\uniform_\ab}=0$ vs. $\totalvardist{\p}{\uniform_\ab}>\dst$? Why not consider 
\begin{equation}
    Z_4 \eqdef \totalvardist{\hat{\p}}{\uniform_\ab}
\end{equation}
the empirical distance to uniform? A reason might be: \emph{this sounds like a terrible idea.} Unless $\ns = \Omega(\ab)$ (which is much more than what we want), we will not have observed most of the domain elements even once, and the empirical distribution $\hat{\p}_\ns$ will be at distance $1-o(1)$ from uniform, \emph{even} if $\p$ is actually uniform. 

That's the thing, though: the devil is in the $o(1)$ details. Sure, $\expect{Z_4}$ will be \emph{almost} $1$ whether $\p$ is uniform or far from it unless $\ns = \Omega(\ab)$. But this ``almost'' will be different in the two cases! Carefully analyzing this tiny gap in expectation, and showing that $Z_4$ concentrates well enough around its expectation to preserve this tiny gap, amazingly leads to a tester with optimal sample complexity $\ns = \Theta(\sqrt{\ab}/\dst^2)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random binary hashing} Now for a tester that is \emph{not} sample-optimal (but has other advantages, and is relatively cute). If there is one thing we know how to do optimally, it's estimating the bias of a coin. We don't have a coin (Bernoulli) here, we have a glorious $(\ab-1)$-dimensional object.
Hell, let's just randomly make it a coin, shall we? Pick your favourite ($4$-wise independent) hash function $h\colon[\ab]\to\{0,1\}$, thus randomly partitioning the domain $[\ab]$ in two sets $S_0,S_1$. Hash all the $\ns$ samples you got: \emph{now} we have a random coin!

Let's estimate its bias then: we know exactly what this should be under the uniform distribution: $\uniform_\ab(S_0)$. If only we could argue that $\p(S_0)$ noticeably differs from $\uniform_\ab(S_0)$ (with high probability over the random choice of the hash function) whenever $\p$ is $\dst$-far from uniform, we'd be good. Turns out\dots it is the case:
\begin{equation}
    \probaDistrOf{S\subseteq [\ab]}{ |\p(S)-\uniform_\ab(S)| = \Omega(\dst/\sqrt{\ab}) } = \Omega(1)
\end{equation}
So we can just do exactly this: we need to estimate the bias $\p(S_0)$ up to an additive $\alpha \asymp \dst/\sqrt{\ab}$. This can be done with $\ns = \Theta(1/\alpha^2)= \Theta(\ab/\dst^2)$ samples, as desired.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bipartite collisions} In the collision-based tester above, we took a multiset $S$ of $\ns$ samples from $\p$, and looked at the number of ``collisions'' in $S$ to define our statistic $Z_1$. That is fine, but requires to keep in memory all the samples observed so far. One related idea would be to instead take \emph{two} multisets $S_1,S_2$ of $\ns_1$ and $\ns_2$ samples, and only count ``bipartite collisions,'' i.e., collisions between a sample of $S_1$ and one of $S_2$:
\begin{equation}
    Z_5 = \frac{1}{\ns_1 \ns_2}\sum_{(x,y)\in S_1\times S_2} \indic{x=y}
\end{equation}
One can check that $\expect{Z_5} = \normtwo{\p}^2$. Back to $\lp[2]$ as proxy! Compared to the ``vanilla'' collision-based test, this is more flexible ($S_1,S_2$ need not be of the same size), and thus lends itself to some settings where a tradeoff between $\ns_1$ and $\ns_2$ is desirable (roughly speaking, one needs $\ns_1\ns_2 \gtrsim \ab/\dst^4$, and the sample complexity is $\ns=\ns_1+\ns_2$). For the case $\ns_1=\ns_2$, this retrieves the optimal $\ns \asymp \sqrt{\ab}/\dst^2$, with some extra technical condition stemming from the analysis, unfortunately: one needs $\dst = \Omega(1/\ab^{1/10})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Empirical subset weighting} That one, I really like. It's adaptive, it's weird, and (I think) it's new. Fix a parameter $1\leq s \leq \ns$. Take $\ns$ samples from $\p$, and consider the set $S$ (not multiset) induced by the first $s$ samples you get. One can check that
\begin{equation}
    \expect{\p(S)} = \sum_{i=1}^\ab \p(i) (1-(1-\p(i))^s)
\end{equation}
which should be roughly (making a bunch of approximations) $\expect{\p(S)}\approx s\normtwo{\p}^2$. Under the uniform distribution, this is exactly $(1-(1-1/\ab)^s)\approx s/\ab$, where the approximation is valid for $s\ll \ab$.

Great: we have a new estimator for (roughly) the $\lp[2]$ norm! Now, assuming things went well, as the end of this first stage we have a set $S$ such that $\p(S)$ is approximately either $s/\ab$ or $s\normtwo{\p}^2\geq s(1+\Omega(\dst^2))/\ab$ (we just argued that this is what things happen \emph{in expectation}).\footnote{Some more details are required to argue that $\p(S)$ does concentrate enough around its expectation.} So, let's do a second stage! Take the next $\ns-s$ samples, and just count the number of them which fall in $S$: this is allows you to estimate $\p(S)$  up to an additive $s\dst^2/\ab$, as long as
\[
      \ns - s \gtrsim \frac{\ab}{s\dst^4}
\]
(exercise: check that). Optimizing, we get that for $s=\ns/2$ this leads to $\ns \asymp \sqrt{\ab}/\dst^2$: optimal sample complexity! Only drawback: we need $s\ll \ab$ for our approximations to be valid (after that, $\expect{\p(S)}$ cannot be approximately $s\normtwo{\p}^2$ anymore; same issue as with the ``unique elements'' algorithm), so we get the condition $\dst \gg 1/\ab^{1/4}$. Slight bummer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Identity testing}
  \label{sec:identity}
\cnote{Discuss the instance-optimal setting as well, and the relation to unifor-
mity testing.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Historical notes}
\cite{Ingster??}
\cite{GoldreichR00}
\cite{Batu++}
\cite{Paninski08}
\cite{ChanDVV14}
\cite{AcharyaDK15}
\cite{DiakonikolasGPP18}
\cite{BlaisCG17}
\cite{HuangM13}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercises}
Prove the monotonicity of $\lp[p]$ norms

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deferred proof}
  \label{sec:deferred:chap:identity}
\begin{lemma}[\cref{lemma:technical:distinct:elements}, restated]
Fix $m \geq 1$ and $\ab \in \N$. For any $x_1,\dots,x_\ab \geq 0$ such that $\sum_{i=1}^\ab x_i = 1$, we have
\[
    \frac{m\sum_{1\leq i < j \leq \ab} x_i x_j \Paren{ (1-x_i - x_j)^{m-1} - (1-x_i)^m(1-x_j)^m} }{\sum_{i=1}^\ab x_i (1-(1-x_i)^m)} \leq 1
\]
\end{lemma}
\begin{proof}
Define $(y_i)_{1\leq i\leq \ab}$ by $y_i \eqdef 1-(1-x_i)^m$, and note that Bernoulli's inequality implies that $y_i \leq m x_i$ for all $i$. In particular, $\sum_{i=1}^\ab y_i \leq m$. Further, since $x_i + x_j \leq 1$ for all $i\neq j$, we have $0 \leq 1-x_i -x_j \leq (1-x_i)(1-x_j)$, which implies $(1-x_i -x_j)^{m-1} \leq (1-x_i)^{m-1}(1-x_j)^{m-1}$.

This lets us bound the numerator as
\begin{align*}
m\sum_{i < j} &x_i x_j \Paren{ (1-x_i - x_j)^{m-1} - (1-x_i)^m(1-x_j)^m} \\
&= \frac{m}{2}\sum_{i \neq j} x_i x_j \Paren{ (1-x_i - x_j)^{m-1} - (1-x_i)^m(1-x_j)^m} \\
&\leq \frac{m}{2}\sum_{i \neq j} x_i x_j (1-x_i)^{m-1}(1-x_j)^{m-1} \Paren{ 1 - (1-x_i)(1-x_j)} \\
&\leq \frac{m}{2}\sum_{i, j} x_i x_j (1-x_i)^{m-1}(1-x_j)^{m-1} \Paren{x_i+x_j}
\end{align*}
To relate this to the denominator, which can be rewritten as $\sum_{i=1}^\ab x_i y_i$, we rely on the following inequality: for every $x\in[0,1]$
\[
1-(1-x)^m = m\int_{0}^x (1-u)^{m-1} \dd{m} \geq m x (1-x)^{m-1}
\]
and so $x_i (1-x_i)^{m-1} \leq \frac{1}{m}y_i$ for all $i$. It follows that
\begin{align*}
m\sum_{i < j} &x_i x_j \Paren{ (1-x_i - x_j)^{m-1} - (1-x_i)^m(1-x_j)^m} \\
&\leq \frac{1}{2m}\sum_{i, j} y_i y_j \Paren{x_i+x_j} 
= \frac{1}{m}\sum_{i=1}^\ab x_i y_i \sum_{j=1}^\ab y_j \\
&\leq \sum_{i=1}^\ab x_i y_i\,,
\end{align*}
concluding the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bgroup\color{black!50!purple}
\paragraph{The variance analysis from~\citet{Paninski08}.}
The proof of~\cref{theo:uniformity:unique:elements} presented in this survey departs from the original one from~\citet{Paninski08}. The argument analyzing the expectation gap (\cref{lemma:gap:z2}) is similar, although we tried to make it a little simpler and intuitive (which, admittedly, is very subjective). The main difference is in bounding the variance in the ``far'' case; indeed, while~\cite{Paninski08} relies for this on the Efron--Stein inequality, the proof he presents is flawed, and the claimed variance bound does not follow.

Specifically, the proof of~\citet[Lemma~2]{Paninski08} claims the bound
\[
  \Var_{\p}[Z_2] \leq \ns \sum_{i=1}^\ab p(i)(1-(1-\p(i))^{\ns-1})
\]
which, if true, would imply (after renormalizing to match our notation)
\[
  \Var_{\uniform_\ab}[Z_2] \leq \frac{1}{\ns}(1-(1-1/\ab)^{\ns-1}) \operatorname*{\sim}_{\ab\to \infty} \frac{1}{\ab}
\]
while the (exact) variance in this case is
\begin{align*}
\Var_{\uniform_\ab}[Z_2]
&= \frac{\bE{\uniform_\ab}{Z_2}(1-\bE{\uniform_\ab}{Z_2})}{\ns} + \frac{\ns-1}{\ns}\Paren{ \frac{\ab-1}{\ab}\Paren{1-\frac{2}{\ab}}^{\ns-2} - \bE{\uniform_\ab}{Z_2}^2} \\
&\operatorname*{\sim}_{\ab\to \infty} \frac{2}{\ab}
\end{align*}
This shows that the former upper bound cannot hold as stated. Looking into the proof, the issue appears to be at the first step in bounding 
the quantity $\frac{1}{2} \sum_{t=1}^\ns \bEE{(S-S^{(t)})^2}$~\cite[p.3, top of right column]{Paninski08}, as $|S-S^{(i)}|$ can take values 0, 1,or 4 (not just 0 or 1), and some events leading to these have been forgotten. As an example: the case $(n_i,n_j)=(0,1)$ on $\ns-1$ samples leads to a difference of 2 when the remaining sample falls on $i$ in one case and $j$ in the other (as it gives 2 unique elements in one case, and 0 in the other), and is thus different from say the case $(n_i,n_j)=(0,2)$ which leads to a difference of 1 when the remaining sample falls on $i$ in one case and $j$ in the other (as it gives 1 unique elements in one case, and 0 in the other). The current analysis omits this distinction, and some other cases; it is not clear to us how to fix their proof, which is why we chose to provide an alternative argument to bound the variance.

\paragraph{More details.} Specifically, after using the Efron--Stein inequality,~\citet{Paninski08} bounds the expected squared difference between $S$ and $S'$ (the number of unique elements observed on $\ns$ samples, where $S$ and $S'$ differ only one one single sample: i.e., $S=S(X_1,\dots,X_{\ns-1}, X_\ns)$ and $S=S(X_1,\dots,X_{\ns-1}, X'_\ns)$, with $X_\ns,X'_\ns$ i.i.d.) as
\[
  \bEE{(S-S')^2} = \sum_{1\leq i,j\leq \ab} \p(i)\p(j) \bE{X_1,\dots,X_{\ns-1}}{ \indic{\ns_i=0, \ns_j>0} + \indic{\ns_j=0, \ns_i>0} }
\]
by considering what $X_\ns,X'_\ns$ take, and the contribution to $(S-S')^2$ this makes (where $\ns_i$ denotes the number of occurrences of $i$ among $X_1,\dots,X_{\ns-1}$).
However, accounting for all the ways $(S-S')^2$ could differ shows that there are 6 terms, not just 2: instead of 
\begin{equation}
  \label{eq:pan:bound}
\indic{\ns_i=0, \ns_j>0} + \indic{\ns_j=0, \ns_i>0} 
\end{equation}
one should have
\begin{align*}
& 4\cdot \textcolor{red}{\indic{\ns_i=0, \ns_j=1}} + \textcolor{red}{\indic{\ns_i=0, \ns_j>1}} + \indic{\ns_i=1, \ns_j>1} \\
&+ 4\cdot \textcolor{red}{\indic{\ns_j=0, \ns_i=1}} + \textcolor{red}{\indic{\ns_j=0, \ns_i>1}} + \indic{\ns_j=1, \ns_i>1} 
\end{align*}
and while~\eqref{eq:pan:bound} (up to a factor 4) can be used to upper bound the sum of 4 of those 6 terms (those in red), there remain 2 unaccounted for:
$
\indic{\ns_i=1, \ns_j>1}
$
and 
$
\indic{\ns_j=1, \ns_i>1}
$. As mentioned above, it is not clear to us how to bound the contribution of those extra two terms (while the other 4 ``red'' terms, of course, would follow from the original argument, losing a factor 4 only in the bound).
\egroup
